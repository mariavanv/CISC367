{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "known-defense",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Fill in your name and the link to this file on your github.\n",
    "\n",
    "* Name: Maria van Venrooy\n",
    "* Link to github URL: https://github.com/mariavanv/CISC367/tree/main/ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enabling-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-credits",
   "metadata": {},
   "source": [
    "# ML: Linear Regression\n",
    "\n",
    "So this starts with linear regression. If you want a deeper dive than what I cover in class, you can refer to [this page](https://realpython.com/linear-regression-in-python/)\n",
    "\n",
    "The exercises come from this workbook, which has somewhat helpful explanations too: https://csmastersuh.github.io/data_analysis_with_python_2020/linear_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-integral",
   "metadata": {},
   "source": [
    "# Exercise 10: Linear Regression\n",
    "\n",
    "You'll need to make up some data for this. Don't spend too much time on this one, it's less interesting compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-glory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.52631579,  1.05263158,  1.57894737,  2.10526316,\n",
       "        2.63157895,  3.15789474,  3.68421053,  4.21052632,  4.73684211,\n",
       "        5.26315789,  5.78947368,  6.31578947,  6.84210526,  7.36842105,\n",
       "        7.89473684,  8.42105263,  8.94736842,  9.47368421, 10.        ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.6914968 ,  1.54691817,  0.89948667,  4.89128808,  5.67740126,\n",
       "        6.60070376,  6.40299345,  8.33193467, 11.74754875, 12.04641005,\n",
       "       11.60503136, 14.02993721, 13.55504124, 14.54131348, 16.32486711,\n",
       "       18.37318219, 18.59975606, 18.75702241, 20.48684856, 21.64860774])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=20\n",
    "# Linearly increasing x values\n",
    "x = np.linspace(0, 10, n)\n",
    "# Wonky line of points\n",
    "y = x*2 + 1 + 1*np.random.randn(n)\n",
    "display(x, y)\n",
    "plt.scatter(x, y)\n",
    "#plt.show()\n",
    "\n",
    "model = np.polyfit(x,y,1)\n",
    "predict = np.poly1d(model)\n",
    "x_lin_reg = x\n",
    "y_lin_reg = predict(x_lin_reg)\n",
    "plt.plot(x_lin_reg, y_lin_reg, c = 'r')\n",
    "plt.show()\n",
    "# Do actual linear regression here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-river",
   "metadata": {},
   "source": [
    "# Exercise 11: Mystery Data\n",
    "\n",
    "This one is far more interesting. You can download the file from [here](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part05-e11_mystery_data/src/mystery_data.tsv). Make sure it gets the right filename!\n",
    "\n",
    "You don't need to define any functions, as they demand, although you might find that helpful to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handed-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of X1 is 3.0000000000000244\n",
      "Coefficient of X2 is -0.9999999999999867\n",
      "Coefficient of X3 is 6.99999999999999\n",
      "Coefficient of X4 is 9.769962616701378e-15\n",
      "Coefficient of X5 is -19.999999999999993\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df11 = pd.read_csv(\"mystery_data.tsv\", sep='\\t')\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(df11.iloc[:,0:5], df.iloc[:,5])\n",
    "\n",
    "coefs = model.coef_\n",
    "\n",
    "for i, co in enumerate(coefs):\n",
    "        print(f\"Coefficient of X{i+1} is {co}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-banking",
   "metadata": {},
   "source": [
    "I think all of the features are needed except for x4, because it's coefficient is reallllyyyy tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-madison",
   "metadata": {},
   "source": [
    "## Exercise 12: Coefficient of Determination\n",
    "\n",
    "Read over this entire problem, parts 1 and 2.\n",
    "\n",
    "This reuses the same `mystery_data.tsv` file as before.\n",
    "\n",
    "Again, you do not need to define their function. Just calculate the R2 scores and print them, as they direct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "certified-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-score with feature(s) X: 1.0\n",
      "R2-score with feature(s) X2: 0.01856867054794764\n",
      "R2-score with feature(s) X3: 0.00027043105921675803\n",
      "R2-score with feature(s) X4: 0.08699933347375\n",
      "R2-score with feature(s) X5: 0.0014204162338299353\n",
      "R2-score with feature(s) X6: 0.8607959696159544\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "df12 = pd.read_csv(\"mystery_data.tsv\", sep=\"\\t\")\n",
    "\n",
    "x = df12.iloc[:,0:5]\n",
    "y = df.iloc[:,5]\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x,y)\n",
    "scores = []\n",
    "scores.append(reg.score(x,y))\n",
    "for i in x:\n",
    "    reg.fit(df[i].values.reshape(-1,1),y)\n",
    "    score = reg.score(df[i].values.reshape(-1,1),y)\n",
    "    scores.append(score)\n",
    "    \n",
    "for i, n in enumerate(scores):\n",
    "    if i==0:\n",
    "        print(f\"R2-score with feature(s) X: {n}\")\n",
    "    else:\n",
    "        print(f\"R2-score with feature(s) X{i+1}: {n}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-launch",
   "metadata": {},
   "source": [
    "## Exercise 13: Cycling Weather\n",
    "\n",
    "I've already prepared the data that they require for this assignment. You can download it [here](https://gist.githubusercontent.com/acbart/466174a04e9a2505c4c25f91fc6dd4f6/raw/726865070677ec7dede17a08095624e0ea35e7cd/biking.csv).\n",
    "\n",
    "The first column is the index, you can safely ignore it. The next 7 columns are straightforward. The last few columns are locations in Finland that have measuring stations. I recommend using `Baana` as they say in the instructions for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "commercial-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baana:\n",
      "Reg coef for 'precipitation':  -52.179548960018785\n",
      "Reg coef for 'snow depth':  -32.93766860685709\n",
      "Reg coef for 'Temperature':  169.24225037357726\n",
      "Score: ' 0.5750246253469835\n"
     ]
    }
   ],
   "source": [
    "df13 = pd.read_csv(\"biking.csv.txt\")\n",
    "\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "\n",
    "X = df13[['Precipitation amount (mm)', 'Snow depth (cm)', 'Air temperature (degC)']]\n",
    "y = df13[['Baana']]\n",
    "\n",
    "\n",
    "reg.fit(X, y)\n",
    "\n",
    "coef = reg.coef_[0]\n",
    "\n",
    "print(\"Baana:\")\n",
    "\n",
    "print(\"Reg coef for 'precipitation': \" , coef[0])\n",
    "print(\"Reg coef for 'snow depth': \" , coef[1])\n",
    "print(\"Reg coef for 'Temperature': \" , coef[2])\n",
    "print(\"Score: '\" , reg.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-airfare",
   "metadata": {},
   "source": [
    "# ML Naive Bayes Classification\n",
    "\n",
    "This is the next section of the exercises, from: https://csmastersuh.github.io/data_analysis_with_python_2020/bayes.html\n",
    "\n",
    "In addition to the reading, I recommend this video: https://www.youtube.com/watch?v=CPqOCI0ahss\n",
    "\n",
    "\n",
    "## Exercise 1: Blob Classification\n",
    "\n",
    "(**OPTIONAL**) This one is very vague, and they're actually asking you to generate your own test data using the `make_blobs` function from `sklearn`'s `datasets` submodule. I've already started that work for you. But honestly if you want to skip it, I don't think it's a helpful starting question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sustained-laser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is None\n",
      "array([[2.0, 2.0, 0.0, 2.5, None],\n",
      "       [2.0, 3.0, 1.0, 1.5, None],\n",
      "       [2.0, 2.0, 6.0, 3.5, None],\n",
      "       [2.0, 2.0, 3.0, 1.2, None],\n",
      "       [2.0, 4.0, 4.0, 2.7, None]], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "def blob_classification(X, y):\n",
    "    # Put ML stuff here\n",
    "    pass\n",
    "\n",
    "# Create the training data and validation data\n",
    "X, y = datasets.make_blobs(100, 2, centers=2, random_state=2, cluster_std=2.5)\n",
    "# Run your ML predictions\n",
    "print(\"The accuracy score is\", blob_classification(X, y))\n",
    "# Run this on some new data\n",
    "a=np.array([[2, 2, 0, 2.5],\n",
    "            [2, 3, 1, 1.5],\n",
    "            [2, 2, 6, 3.5],\n",
    "            [2, 2, 3, 1.2],\n",
    "            [2, 4, 4, 2.7]])\n",
    "accuracies = []\n",
    "for row in a:\n",
    "    X,y = datasets.make_blobs(100, int(row[0]), centers=int(row[1]),\n",
    "                              random_state=int(row[2]), cluster_std=row[3])\n",
    "    accuracies.append(blob_classification(X, y))\n",
    "print(repr(np.hstack([a, np.array(accuracies)[:,np.newaxis]])))\n",
    "# The last column should be the categorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-compromise",
   "metadata": {},
   "source": [
    "## Exercise 2: Plant Classification\n",
    "\n",
    "This is a much better question. The Iris dataset is a classic: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "The wikipedia page gives an example of how to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "original-vienna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "modelplant = naive_bayes.GaussianNB()\n",
    "modelplant.fit(X_train, y_train)\n",
    "\n",
    "plantpredict = modelplant.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \" , metrics.accuracy_score(y_test, plantpredict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-flooring",
   "metadata": {},
   "source": [
    "## Exercise 3: Word Classification\n",
    "\n",
    "(**Skip**)\n",
    "\n",
    "This one is too much. They give some of the data as an XML file. It's an interesting problem, and you can find the data (and solution) [here](https://github.com/AnkS4/hy-data-analysis-with-python-2020/tree/master/part06-e03_word_classification/src) if you want to tackle it, but I'm skipping it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skip!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-israel",
   "metadata": {},
   "source": [
    "## Exercise 4: Spam Detection\n",
    "\n",
    "Download [ham.txt.gz](https://github.com/AnkS4/hy-data-analysis-with-python-2020/raw/master/part06-e04_spam_detection/src/ham.txt.gz) and [spam.txt.gz](https://github.com/AnkS4/hy-data-analysis-with-python-2020/raw/master/part06-e04_spam_detection/src/spam.txt.gz).\n",
    "\n",
    "This one is much more interesting and reasonable. It requires processing some large text files, but that's actually the easiest part, as shown in the code below. The idea is that you have spam (bad emails) and ham (good emails), and you want to determine which is which. I've done similar email processing (detecting job ads for a conference) and I was impressed with how easily I could train a little data and get very good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "seven-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails loaded as strings: 500\n",
      "Number of ham emails loaded as strings: 2500\n",
      "Accuracy score: 0.9693333333333334\n",
      "23 messages miclassified out of 750\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the spam emails as strings in a list.\n",
    "with gzip.open('spam.txt.gz', 'rb') as spam_file:\n",
    "    spam = spam_file.readlines()\n",
    "print(\"Number of spam emails loaded as strings:\", len(spam))\n",
    "\n",
    "# Now do the same thing with the `ham.txt.gz`\n",
    "with gzip.open('ham.txt.gz', 'rb') as ham_file:\n",
    "    ham = ham_file.readlines()\n",
    "print(\"Number of ham emails loaded as strings:\", len(ham))\n",
    "\n",
    "# And then do the actual ML stuff\n",
    "X = ham + spam\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "y = np.zeros(len(X))\n",
    "y[len(ham):] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.75)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "miss = (y_test != y_pred).sum()\n",
    "\n",
    "print(\"Accuracy score:\", score)\n",
    "print(f\"{miss} messages miclassified out of {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-amendment",
   "metadata": {},
   "source": [
    "# ML Clustering\n",
    "\n",
    "This is the last section: https://csmastersuh.github.io/data_analysis_with_python_2020/clustering.html\n",
    "\n",
    "This section is one of the most interesting in my opinion. K-Means is a pretty straightforward tool, and is really worth learning how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-motel",
   "metadata": {},
   "source": [
    "## Exercise 5: Plant Clustering\n",
    "\n",
    "Same deal as before; use the IRIS dataset. Since this has so many parameters, it can be tricky to make a good visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "expressed-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clustmodel = KMeans(3, random_state=0)\n",
    "clustmodel.fit(X)\n",
    "\n",
    "\n",
    "perm = []\n",
    "\n",
    "for i in range(3):\n",
    "        idx =  clustmodel.labels_ == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(y[idx])[0][0]\n",
    "        perm.append(new_label)\n",
    "        \n",
    "new_labels = [perm[label] for label in clustmodel.labels_]\n",
    "        \n",
    "        \n",
    "print(\"Accuracy: \" , accuracy_score(y,new_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-thesis",
   "metadata": {},
   "source": [
    "## Exercise 6: Non-convex Clusters\n",
    "\n",
    "The data for this question is [here](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part06-e06_nonconvex_clusters/src/data.tsv).\n",
    "\n",
    "This one shows off a different clustering algorithm ([`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)), which is \"Good for data which contains clusters of similar density\". I wasn't very familiar with DBSCAN, but it does seem much better than KMeans. It doesn't require you to figure out the number of clusters, and seems to be tricked less by unusual data. [This page](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html) was very helpful in breaking that difference down.\n",
    "\n",
    "The reference answer uses a `for` loop and `np.arange` to try `e` values from 0.05 to 0.2 in 0.05 increments, but I don't mind if you just manually try some different `e` values.\n",
    "\n",
    "Please do make a visualization with clusters colored, since I think that really highlights what we are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "owned-profit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eps  Score  Clusters  Outliers\n",
      "0  0.10    NaN       2.0       3.0\n",
      "1  0.15    NaN       2.0       0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "df6 = pd.read_csv(\"data.tsv.txt\", sep=\"\\t\")\n",
    "X = df6[[\"X1\",\"X2\"]]\n",
    "y = df6[\"y\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for e in np.arange(0.05, 0.2, 0.05):\n",
    "    model = DBSCAN(e)\n",
    "    model.fit(X)\n",
    "    idx = model.labels_ == -1\n",
    "    outliers = np.sum(idx)\n",
    "    clusters = max(model.labels_) + 1\n",
    "    \n",
    "    if clusters == len(y.unique()):\n",
    "        #permutation = find_permutation(clusters, y, model.labels_)\n",
    "        permutation=[]\n",
    "        for i in range(clusters):\n",
    "            idx = model.labels_ == i\n",
    "            # Choose the most common label among data points in the cluster\n",
    "            new_label=scipy.stats.mode(y[idx])[0][0]\n",
    "            permutation.append(new_label)\n",
    "        \n",
    "        \n",
    "            score = accuracy_score(y[idx], [permutation[label] for label in model.labels_[idx]])\n",
    "        else:\n",
    "            score = np.nan\n",
    "        results.append([e, score, clusters, outliers])\n",
    "        \n",
    "print(pd.DataFrame(data=results, columns=[\"eps\", \"Score\", \"Clusters\", \"Outliers\"], dtype=\"float64\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-somalia",
   "metadata": {},
   "source": [
    "## Exercise 7: Binding Sites\n",
    "\n",
    "Download the [`data.seq` file](https://raw.githubusercontent.com/AnkS4/hy-data-analysis-with-python-2020/master/part06-e07_binding_sites/src/data.seq); note that it is just a plain textual data file, despite the fancy extension.\n",
    "\n",
    "They ask you to define `get_features_and_labels` to accept a filename, even though there's only one test file. Up to you if you want to hardcode the file path in or make it a flexible function.\n",
    "\n",
    "There are multiple parts here, and they ask you to compare the euclidean and hamming distance. I think it's worth thinking about - if you don't get what they mean, do ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "gentle-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The `find_permutation` function provided in the text, for your convenience\n",
    "def find_permutation(n_clusters, real_labels, labels):\n",
    "    permutation=[]\n",
    "    for i in range(n_clusters):\n",
    "        idx = labels == i\n",
    "        # Choose the most common label among data points in the cluster\n",
    "        new_label=scipy.stats.mode(real_labels[idx])[0][0]\n",
    "        permutation.append(new_label)\n",
    "    return permutation\n",
    "\n",
    "#this one looks like a lot and I think my time will be better spend working more on the final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-robertson",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
